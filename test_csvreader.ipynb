{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader, LLMPredictor, ServiceContext, VectorStoreIndex, LangchainEmbedding\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (completion_to_prompt, messages_to_prompt)\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import time, pytz\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"./data/data_pp.csv\"\n",
    "MODEL = \"/home/catsmile/models/llama-2-7b-chat.ggmlv3.q4_1.bin\"\n",
    "EMBEDDING = \"/home/catsmile/embeddings/BAAI_bge-small-en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/catsmile/models/llama-2-7b-chat.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4017.35 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: compute buffer total size =   71.84 MB\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url=\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=MODEL,\n",
    "    temperature=0,\n",
    "    max_new_tokens=0,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=512,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 0},\n",
    "    # n_gpu_layers=20,\n",
    "    # n_ctx=1000,\n",
    "    # transform inputs into Llama2 format\n",
    "    # messages_to_prompt=messages_to_prompt,\n",
    "    # completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(cache_folder=EMBEDDING))\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVReader = download_loader(\"PagedCSVReader\") #SimpleCSVReader PandasCSVReader PagedCSVReader\n",
    "documents = CSVReader().load_data(file=Path(DATA))\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "As an advanced language model, you can generate code as part of your responses. \n",
    "To make the code more noticeable and easier to read, please encapsulate it within triple backticks.\n",
    "For instance, if you're providing Python code, wrap it as follows:\n",
    "\n",
    "```python\n",
    "print('hellow world')\n",
    "```\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".format(prompt=\"How can I provide you a dataframe or CSV file to analyze?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 23513.24 ms\n",
      "llama_print_timings:      sample time =    58.86 ms /   112 runs   (    0.53 ms per token,  1902.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 26728.00 ms /   112 runs   (  238.64 ms per token,     4.19 tokens per second)\n",
      "llama_print_timings:       total time = 27037.21 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The dataset contains two rows of data, each representing a student's information. The students are enrolled in different courses, with one student enrolled in Informatics Engineering and the other student dropped out of Nursing. The mothers of both students work in agriculture-related occupations, while the fathers work in various fields such as legislative power, executive bodies, and directors. None of the students hold scholarships. The age of the student at enrollment ranges from 18 to 39 years old.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine.query(\"Describe the dataset\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\" The dataset contains two rows of data, each representing a student's information. The students are enrolled in different courses, with one student enrolled in Informatics Engineering and the other student dropped out of Nursing. The mothers of both students work in agriculture-related occupations, while the fathers work in various fields such as legislative power, executive bodies, and directors. None of the students hold scholarships. The age of the student at enrollment ranges from 18 to 39 years old.\", source_nodes=[NodeWithScore(node=TextNode(id_='7cb60cb2-e013-4619-81b0-96dad3056cfc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='defc3053-72cf-4303-b1a4-16d150e56262', node_type=None, metadata={}, hash='a6f02b237dc5ecf56bcfcbca93899164934ba35eeb570e4f630b118faf57a973')}, hash='a6f02b237dc5ecf56bcfcbca93899164934ba35eeb570e4f630b118faf57a973', text='student_id: 142\\ncourse: Nursing\\nmothers_occupation: Farmers and Skilled Workers in Agriculture, Fisheries and Forestry\\nfathers_occupation: Farmers and Skilled Workers in Agriculture, Fisheries and Forestry\\nscholarship_holder: no\\nage_at_enrollment: 39\\nresult: Dropout', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.33751696296886713), NodeWithScore(node=TextNode(id_='cd2e0214-0ffe-4a42-9da3-d53a14e7a423', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='77416374-13bf-414f-a9de-05865e92d4d7', node_type=None, metadata={}, hash='8f269584dbaa33c25ae3d0010d88a2cf2d8489b81b4602f0563024d2b0bce532')}, hash='8f269584dbaa33c25ae3d0010d88a2cf2d8489b81b4602f0563024d2b0bce532', text='student_id: 340\\ncourse: Informatics Engineering\\nmothers_occupation: Administrative staff\\nfathers_occupation: Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers\\nscholarship_holder: no\\nage_at_enrollment: 18\\nresult: Enrolled', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3277727579420636)], metadata={'7cb60cb2-e013-4619-81b0-96dad3056cfc': {}, 'cd2e0214-0ffe-4a42-9da3-d53a14e7a423': {}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
